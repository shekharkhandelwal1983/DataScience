{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form all the URLs to Parse\n",
    "root_url=\"https://www.prokabaddi.com/stats/\"\n",
    "player_matrix=['102-total-points-statistics' , \n",
    "               '21-successful-raids-statistics' , \n",
    "               '22-raid-points-statistics' , \n",
    "               '23-successful-tackles-statistics' , \n",
    "               '103-tackle-points-statistics' , \n",
    "               '138-avg-raid-points-statistics' , \n",
    "               '139-avg-tackle-points-statistics' , \n",
    "               '132-do-or-die-raid-points-statistics' , \n",
    "               '104-super-raids-statistics' , \n",
    "               '28-super-tackles-statistics' , \n",
    "               '100-super-10s-statistics' , \n",
    "               '101-high-5s-statistics'\n",
    "              ]\n",
    "team_matrix = [ '134-super-raid-statistics' , \n",
    "                '20-super-tackles-statistics' , \n",
    "                '136-all-outs-inflicted-statistics' , \n",
    "                '137-all-outs-conceded-statistics'\n",
    "              ]\n",
    "\n",
    "season_list = [x for x in [[1,2,3,4,8,26,49]]\n",
    "team_mat_target_urls=[root_url+str(season)+'-'+team_mat for season in season_list for team_mat in team_matrix]\n",
    "player_mat_target_urls=[root_url+str(season)+'-'+play_mat for season in season_list for play_mat in player_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_source = {}\n",
    "info = pd.DataFrame(columns=('rank', 'name','type', 'matchs_played','score', 'id', 'profile_name', 'team', 'season', 'matrix_code', 'matrix_name' ))\n",
    "def get_full_source_player(url):\n",
    "    season_details = url.split('/')[-1].split('-',2)\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\sumsatap\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(1)\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@id=\"load_more\"]/span').click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            page_src = driver.page_source\n",
    "            soup = BeautifulSoup(page_src, 'lxml')\n",
    "            div_span = []\n",
    "            div_a = []\n",
    "            for div in soup.find_all('div', {\"class\": \"sipk-lb-detailItem wl-team-detail\"}):\n",
    "                div_span.append(div.find_all('span'))\n",
    "                ous=[]\n",
    "                for i in range(len(div_span)):\n",
    "                    ins=[]\n",
    "                    for elem in div_span[i]:\n",
    "                        ins.append(elem.text.replace('\\n','').strip())\n",
    "                    info.loc[i,:len(ins)]=ins\n",
    "                div_a.append(div.find_all('a'))\n",
    "                for i in range(len(div_a)):\n",
    "                    ins=[]\n",
    "                    for a in div_a[i]:\n",
    "                        ins.append(a.attrs['href'])\n",
    "                    #print(ins)\n",
    "                    info.loc[i,5:8]=ins\n",
    "            info[['season', 'matrix_code', 'matrix_name']] = season_details\n",
    "            info.to_csv('_'.join(season_details)+'.csv')\n",
    "            driver.quit()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in player_mat_target_urls:\n",
    "    get_full_source_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.DataFrame(columns=('rank', 'team', 'matchs_played','score', 'id', 'profile_name', 'season', 'matrix_code', 'matrix_name' ))\n",
    "def get_full_source_team(url):\n",
    "    season_details = url.split('/')[-1].split('-',2)\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\sumsatap\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(1)\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@id=\"load_more\"]/span').click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            page_src = driver.page_source\n",
    "            soup = BeautifulSoup(page_src, 'lxml')\n",
    "            div_span = []\n",
    "            div_a = []\n",
    "            for div in soup.find_all('div', {\"class\": \"sipk-lb-detailItem wl-team-detail\"}):\n",
    "                div_span.append(div.find_all('span'))\n",
    "                ous=[]\n",
    "                for i in range(len(div_span)):\n",
    "                    ins=[]\n",
    "                    for elem in div_span[i]:\n",
    "                        ins.append(elem.text.replace('\\n','').strip())\n",
    "                    #print(ins)\n",
    "                    info.loc[i,:len(ins)]=ins\n",
    "                div_a.append(div.find_all('a'))\n",
    "                for i in range(len(div_a)):\n",
    "                    ins=[]\n",
    "                    for a in div_a[i]:\n",
    "                        ins.append(a.attrs['href'])\n",
    "                    #print(ins)\n",
    "                    info.loc[i,4:6]=ins\n",
    "            info[['season', 'matrix_code', 'matrix_name']] = season_details\n",
    "            info.to_csv('_'.join(season_details)+'.csv')\n",
    "            driver.quit()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in team_mat_target_urls:\n",
    "    get_full_source_team(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matrix={}\n",
    "for pmat in player_matrix:\n",
    "    code, matrix=pmat.split('-',1)\n",
    "    all_matrix['player_'+matrix]=(pd.concat([pd.read_csv(f) for f in glob.glob('*_'+code+'_*.csv')], ignore_index = False).iloc[:,1:])\n",
    "for tmat in team_matrix:\n",
    "    code, matrix=tmat.split('-',1)\n",
    "    all_matrix['team_'+matrix]=(pd.concat([pd.read_csv(f) for f in glob.glob('*_'+code+'_*.csv')], ignore_index = False).iloc[:,1:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in all_matrix.keys():\n",
    "    curr_df = all_matrix.get(table_name)\n",
    "    curr_df = curr_df[curr_df.isnull().sum(axis=1) == 0]\n",
    "    all_matrix[table_name] = curr_df\n",
    "    \n",
    "    if table_name.startswith('team'):\n",
    "        \n",
    "        split_df = curr_df['profile_name'].str.split(\"-profile-\", n = 1, expand = True)\n",
    "        curr_df[\"profile_name\"] = split_df[0]\n",
    "        curr_df[\"team_id\"] = split_df[1]\n",
    "    if table_name.startswith('player'):\n",
    "        \n",
    "        split_df = curr_df['profile_name'].str.split(\"-profile-\", n = 1, expand = True)\n",
    "        curr_df[\"profile_name\"] = split_df[0]\n",
    "        curr_df[\"player_id\"] = split_df[1]\n",
    "        \n",
    "        split_df = curr_df['team'].str.split(\"-profile-\", n = 1, expand = True)\n",
    "        curr_df[\"team_name\"] = split_df[0]\n",
    "        curr_df[\"team_id\"] = split_df[1]\n",
    "        curr_df = curr_df.drop('team',axis=1)\n",
    "        \n",
    "    curr_df = curr_df.drop(['id','matrix_code','matrix_name'],axis=1)\n",
    "    curr_df.loc[curr_df['season'] == 8, 'season'] = 5\n",
    "    curr_df.loc[curr_df['season'] == 26, 'season'] = 6\n",
    "    curr_df.loc[curr_df['season'] == 49, 'season'] = 7\n",
    "    \n",
    "    all_matrix[table_name] = curr_df\n",
    "    curr_df.to_csv(table_name+'_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
